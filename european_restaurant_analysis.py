# -*- coding: utf-8 -*-
"""European Restaurant Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yA6o3reGDgKXSh5XkUwIqo-oAvntIxs1

#European Restaurant Analysis

##Index


*   Refrences
*   Loading the dataset
*   Cleaning And Pre-processing
*   Exploratory Data Analysis
*   Creating Master Dataframe
*   Temporal and Spatial Analysis
*   Building a Recommendation System

#Dataset

European  Restaurant Data obtained from (https://www.tripadvisor.com/ShowTopic-g1-i12105-k10292711-Datasets_from_tripadvisor-Tripadvisor_Support.html)

# ðŸ”¨ Tools Used:

Numpy and pandas for data manipulation

Seaborn and plotly for data visualization

Folium and geo-pandas for spatial analysis

KNN and cosine similarity for recommendation system

#Refrences

##Academic Refrences:

1. Fukuhara, T., Tenmoku, R., Okuma, T., Ueoka, R., Takehara, M., Kurata, T. (2014).
Improving Service Processes Based on Visualization of Human-Behavior and POS Data:
A Case Study in a Japanese Restaurant. In: Mochimaru, M., Ueda, K., Takenaka, T. (eds)
Serviceology for Services. ICServ 2013. Springer, Tokyo.
https://doi.org/10.1007/978-4-431-54816-4_1
https://link.springer.com/chapter/10.1007/978-4-431-54816-4_1#citeas
2. Zacharopoulou, Dimitra, Andriani Skopeliti, and Byron Nakos. 2021. "Assessment and
Visualization of OSM Consistency for European Cities" ISPRS International Journal of
Geo-Information 10, no. 6: 361. https://doi.org/10.3390/ijgi10060361
3. Assuncao, R. M., Neves, M. C., Camara, G., & Da Costa Freitas, C. (2006). Efficient
regionalization techniques for socio-economic geographical units using minimum spanning trees.
International Journal of Geographical Information Science, 20(7), 797â€“811.
4. Austin, S. B., Melly, S. J., Sanchez, B. N., Patel, A., Buka, S., & Gortmaker, S. L. (2005).
Clustering of fast-food restaurants around schools: A novel application of spatial statistics to the
study of food environments. American Journal of Public Health, 95(9), 1575â€“1581.
5. F.O. Isinkaye, Y.O. Folajimi, B.A. Ojokoh,Recommendation systems: Principles, methods and
evaluation,Egyptian Informatics Journal,Volume 16, Issue 3,2015,Pages 261-273,ISSN
1110-8665, https://doi.org/10.1016/j.eij.2015.06.005
6. Ridhwan M M, Wicaksono G, Nurliana L, Bary P, Suryani F T and Satyanugroho R 2015 Analisis
Daya Saing dan Strategi Industri Nasional di Era Masyarakat Ekonomi ASEAN dan
Perdagangan Bebas (Jakarta, Indonesia: Bank Indonesia)
7. T. Ahmed, L. Akhter, F. R. Talukder, Hasan-Al-Monsur, H. Rahman and A. Sattar, "Restaurant
Recommendation System in Dhaka City using Machine Learning Approach," 2021 10th
International Conference on System Modeling & Advancement in Research Trends (SMART),
MORADABAD, India, 2021, pp. 59-63, doi: 10.1109/SMART52563.2021.9676197.
https://ieeexplore.ieee.org/document/9676197
8. A. Gupta and K. Singh, "Location based personalized restaurant recommendation system for
mobile environments," 2013 International Conference on Advances in Computing,
Communications and Informatics (ICACCI), Mysore, India, 2013, pp. 507-511, doi:
10.1109/ICACCI.2013.6637223. https://ieeexplore.ieee.org/document/6637223
9. T. Horozov, N. Narasimhan and V. Vasudevan, "Using location for personalized POI
recommendations in mobile environments," International Symposium on Applications and the
Internet (SAINT'06), Phoenix, AZ, USA, 2006, pp. 6 pp.-129, doi: 10.1109/SAINT.2006.55.
10. Rehman, F., Khalid, O., & Madani, S. (2017). A comparative study of location-based
recommendation systems. The Knowledge Engineering Review, 32, E7.
doi:10.1017/S0269888916000308
11. M. B. Hossain and M. S. Arefin, "Developing a Framework for Next Point-of-interest
Recommendation from Spatiotemporal Data," 2022 International Conference on
Advancement in Electrical and Electronic Engineering (ICAEEE), Gazipur, Bangladesh,
2022, pp. 1-5, doi: 10.1109/ICAEEE54957.2022.9836471.
12. Ekrem Saralioglu & Oguz Gungor. (2022) Crowdsourcing-based application to solve the
problem of insufficient training data in deep learning-based classification of satellite
images. Geocarto International 37:18, pages 5433-5452.
13. T. Liang, S. Lu and Q. Liu, "Data Visualization System Based on Big Data Analysis," 2020
International Conference on Robots & Intelligent System (ICRIS), Sanya, China, 2020, pp. 76-79,
doi: 10.1109/ICRIS52159.2020.00027
14. Muskan, G. Singh, J. Singh and C. Prabha, "Data Visualization and its Key Fundamentals: A
Comprehensive Survey," 2022 7th International Conference on Communication and Electronics
Systems (ICCES), Coimbatore, India, 2022, pp. 1710-1714, doi:
10.1109/ICCES54183.2022.9835803.
15. T. H. A. Soliman, S. A. El-Moemen Mohamed and A. A. Sewisy, "Developing a mobile
location-based collaborative Recommender System for GIS applications," 2015 Tenth
International Conference on Computer Engineering & Systems (ICCES), Cairo, Egypt, 2015, pp.
267-273, doi: 10.1109/ICCES.2015.7393058. https://ieeexplore.ieee.org/document/6637223
16. Chu, WT., Tsai, YL. A hybrid recommendation system considering visual information for
predicting favorite restaurants. World Wide Web 20, 1313â€“1331 (2017).
https://doi.org/10.1007/s11280-017-0437-1
17. J. Zeng, F. Li, H. Liu, J. Wen and S. Hirokawa, "A Restaurant Recommender System Based on
User Preference and Location in Mobile Environment," 2016 5th IIAI International Congress on
Advanced Applied Informatics (IIAI-AAI), Kumamoto, Japan, 2016, pp. 55-60, doi:
10.1109/IIAI-AAI.2016.126.
18. M. Malathi, S. Sarujith and S. Menon, "Rating-based Restaurant and Food Recommendation
System using Nearest Neighbor Algorithm," 2022 Sixth International Conference on I-SMAC
(IoT in Social, Mobile, Analytics and Cloud) (I-SMAC), Dharan, Nepal, 2022, pp. 876-879, doi:
10.1109/I-SMAC55078.2022.9987306.
https://ieeexplore.ieee.org/document/9987306/authors#authors
19. Citation D L Widaningrum et al 2018 IOP Conf. Ser.: Earth Environ. Sci. 145 012102
DOI 10.1088/1755-1315/145/1/012102

##Non-Academic Refrences:

1. https://www.kaggle.com/datasets/stefanoleone992/tripadvisor-european-restaurants/data
2. https://www.kaggle.com/code/cindyhooo/tripadvisor-european-restaurant
3. https://www.kaggle.com/code/shtrausslearning/geospatial-data-visualisation-australia

#Installing libraries
"""

!pip install geopandas==0.10.2

!pip install mapclassify

import pandas as pd
import gdown
pd.set_option('display.max_columns', None)
pd.options.mode.chained_assignment = None  # default='warn'

import warnings
warnings.filterwarnings('ignore')

gdown.download('https://drive.google.com/uc?id=1GSehPVV2diF9Lub_Vlq3cV2Q2j-C6HnP')

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv('tripadvisor_european_restaurants.csv', encoding='utf8', low_memory=False)

df.head()

"""#Data Cleaning and Pre-processing

Editing certain rows, adding few columns and deleting few
"""

df= df.dropna(subset=['latitude','longitude','original_open_hours', 'cuisines','price_range','avg_rating','total_reviews_count'])

df['original_open_hours']=df['original_open_hours'].fillna({"Mon": ["07:00-14:30", "17:00-23:00"], "Tue": ["07:00-14:30", "17:00-23:00"], "Wed": ["07:00-14:30", "17:00-23:00"], "Thu": ["07:00-14:30", "17:00-23:00"], "Fri": ["07:00-14:30", "17:00-23:00"], "Sat": ["08:00-14:30", "17:00-23:00"], "Sun": ["08:00-14:30"]})
df['open_days_per_week']=df['open_days_per_week'].fillna(7)
df['open_hours_per_week']=df['open_hours_per_week'].fillna(86.5)
df['meals']=df['meals'].fillna('Breakfast, Lunch, Dinner')
df['cuisines']=df['cuisines'].fillna('Multi')
df['minimum_range'] = pd.to_numeric(df['price_range'].str.split('-').str[0].str.replace('â‚¬', '').str.replace(',', ''), errors='coerce')
df['maximum_range'] = pd.to_numeric(df['price_range'].str.split('-').str[1].str.replace('â‚¬', '').str.replace(',', ''), errors='coerce')
df['avg_price'] = (df['minimum_range'] + df['maximum_range']) / 2
df.drop(['minimum_range', 'maximum_range','working_shifts_per_week'], axis=1, inplace=True)

columns_to_drop = ['original_open_hours', 'vegan_options', 'gluten_free', 'restaurant_link', 'original_location',  'province', 'address', 'claimed', 'popularity_detailed', 'popularity_generic', 'special_diets', 'features', 'original_open_hours', 'reviews_count_in_default_language', 'excellent', 'very_good', 'average', 'poor', 'terrible', 'food', 'service', 'value', 'atmosphere', 'keywords' ,'default_language']
df = df.drop(columns=columns_to_drop)

df= df[df['avg_price'] <= 1000].dropna(subset=['avg_price'])

countries_dict = {'Austria': 'AUT', 'Belgium': 'BEL', 'Bulgaria': 'BGR', 'Croatia': 'HRV', 'Czech Republic': 'CZE',
                  'Denmark': 'DNK', 'England': 'GBR', 'Finland': 'FIN', 'France': 'FRA', 'Germany': 'DEU',
                  'Greece': 'GRC', 'Hungary': 'HUN', 'Ireland': 'IRL', 'Italy': 'ITA', 'Northern Ireland': 'GBR',
                  'Poland': 'POL', 'Portugal': 'PRT', 'Romania': 'ROU', 'Scotland': 'GBR', 'Slovakia': 'SVK',
                  'Spain': 'ESP', 'Sweden': 'SWE', 'The Netherlands': 'NLD', 'Wales': 'GBR'}
df['country_code'] = df['country'].map(countries_dict).fillna(df['country'])

df.replace({"â‚¬": 'low', "â‚¬â‚¬-â‚¬â‚¬â‚¬": 'mid', "â‚¬â‚¬â‚¬â‚¬": 'high'}, inplace = True)
df["price_level"].fillna("low")

df['lat_long'] = df[['latitude', 'longitude']].values.tolist()

"""# Exploratory Data Analysis

"""

import folium
from folium import plugins
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
import plotly.express as px
import requests
import json
import folium

"""Heat-map to show the insights of the data-set"""

m = folium.Map(location=(15,30), tiles="OpenStreetMap", zoom_start = 3)
plugins.HeatMap(df['lat_long'], radius=20).add_to(m)
m

"""##Counting number of restaurant in each country"""

sns.set(style='whitegrid')

countries_count = df['country'].value_counts()

plt.figure(figsize=(15, 8))
plt.title('Count of Restaurants by Country', size=20)
ax = sns.barplot(x=countries_count.values, y=countries_count.index, palette='viridis')
plt.xlabel('Number of Restaurants', size=15)
plt.ylabel('Country', size=15)

for i, v in enumerate(countries_count.values):
    plt.text(x=v + 5, y=i + 0.2, s=str(v), color='k', fontsize=12)
ax.xaxis.grid(True)

plt.show()

"""Creating a dataset with important colums for visualizing important aspects related to the restaurent for major country in europe

"""

agg_countries_df = df.groupby('country').agg(
    total_restaurants=pd.NamedAgg(column='restaurant_name', aggfunc='size'),
    avg_rating=pd.NamedAgg(column='avg_rating', aggfunc=np.mean),
    total_reviews=pd.NamedAgg(column='total_reviews_count', aggfunc=np.sum),
    avg_price=pd.NamedAgg(column='avg_price', aggfunc=np.mean),
    open_days_per_week=pd.NamedAgg(column='open_days_per_week', aggfunc=np.mean),
    open_hours_per_week=pd.NamedAgg(column='open_hours_per_week', aggfunc=np.mean),
    latitude=pd.NamedAgg(column='latitude', aggfunc=np.mean),
    longitude=pd.NamedAgg(column='longitude', aggfunc=np.mean)
).reset_index().sort_values(by='total_restaurants', ascending=False)

# Round numeric columns
numeric_columns = ['avg_rating', 'total_reviews', 'avg_price', 'open_days_per_week', 'open_hours_per_week', 'latitude', 'longitude']

agg_countries_df[numeric_columns] = agg_countries_df[numeric_columns].round(3)

#Format latitude and longitude for better readability
agg_countries_df['latitude'] = agg_countries_df['latitude'].apply(lambda x: f'{x:.6f}')
agg_countries_df['longitude'] = agg_countries_df['longitude'].apply(lambda x: f'{x:.6f}')

agg_countries_df.head()

import plotly.express as px

fig = px.treemap(agg_countries_df,
                 path=[px.Constant('Europe'), 'country'],
                 values='total_restaurants',
                 color='avg_rating',
                 hover_data=['total_restaurants', 'avg_rating'],
                 width=800, height=500,
                 labels={'total_restaurants': 'Total Restaurants'},
                 title='Treemap of European countries colored by Average Rating',
                 color_continuous_scale='Viridis')

fig.update_layout(margin=dict(l=10, r=10, t=40, b=15))

fig.show()

geojson_url = 'https://raw.githubusercontent.com/datasets/geo-countries/master/data/countries.geojson'
geojson_data = requests.get(geojson_url).json()

"""Map Visualization"""

m = folium.Map(location=[51.216505, 9.879729], zoom_start=4, tiles="cartodbpositron")

choropleth = folium.Choropleth(
    geo_data=geojson_data,
    name='choropleth',
    data=agg_countries_df,
    columns=['country', 'total_restaurants'],
    key_on='feature.properties.ADMIN',
    fill_color='viridis',
    fill_opacity=0.7,
    line_opacity=0.2,
    legend_name='Total Restaurants',
).add_to(m)

folium.LayerControl().add_to(m)
# Show the map
m

"""##European map of countries based on Open Days per week"""

m = folium.Map(location=[51.216505, 9.879729], zoom_start=4, tiles="cartodbpositron")

choropleth = folium.Choropleth(
    geo_data=geojson_data,
    name='choropleth',
    data=agg_countries_df,
    columns=['country', 'open_days_per_week'],
    key_on='feature.properties.ADMIN',
    fill_color='viridis',
    fill_opacity=0.7,
    line_opacity=0.2,
    legend_name='Restaurants based on Open Days per week ',
).add_to(m)

folium.LayerControl().add_to(m)
# Show the map
m

"""Here we can see that almost all the restaurants in Italy are open approximatly 6 days per week.

##Counting number of restaurant in top 20 countries from the whole europe
"""

import seaborn as sns
import matplotlib.pyplot as plt

# Count of restaurant cities
sns.set(style='whitegrid')

top20_cities = df['city'].value_counts().nlargest(20)

plt.figure(figsize=(15, 8))
plt.title('Top 20 Cities with the Most Restaurants', size=20)
ax = sns.barplot(x=top20_cities.values, y=top20_cities.index, palette='magma')
plt.xlabel('Number of Restaurants', size=15)
plt.ylabel('City', size=15)

for i, v in enumerate(top20_cities.values):
    plt.text(x=v + 5, y=i + 0.2, s=str(v), color='k', fontsize=12)

ax.xaxis.grid(True)

plt.show()

"""Creating a dataset with important colums for visualizing important aspects related to the restaurent for top 20 cities in Europe"""

top20_cities = df['city'].value_counts().nlargest(20).index
top20_cities_df = df[df['city'].isin(top20_cities)]

agg_top20_cities_df = top20_cities_df.groupby(['country', 'city']).agg(
    total_restaurants=pd.NamedAgg(column='restaurant_name', aggfunc='size'),
    avg_rating=pd.NamedAgg(column='avg_rating', aggfunc=np.mean),
    total_reviews=pd.NamedAgg(column='total_reviews_count', aggfunc=np.sum),
    avg_price=pd.NamedAgg(column='avg_price', aggfunc=np.mean),
    open_days_per_week=pd.NamedAgg(column='open_days_per_week', aggfunc=np.mean),
    open_hours_per_week=pd.NamedAgg(column='open_hours_per_week', aggfunc=np.mean),
    latitude=pd.NamedAgg(column='latitude', aggfunc=np.mean),
    longitude=pd.NamedAgg(column='longitude', aggfunc=np.mean)
).reset_index().sort_values(by='total_restaurants', ascending=False).head(20)

numeric_columns = ['avg_rating', 'total_reviews', 'avg_price', 'open_days_per_week', 'open_hours_per_week', 'latitude', 'longitude']
agg_top20_cities_df[numeric_columns] = agg_top20_cities_df[numeric_columns].round(3)

import plotly.express as px

fig = px.treemap(agg_top20_cities_df,
                 path=[px.Constant('Europe'), 'country', 'city'],
                 values='total_restaurants',
                 color='avg_rating',
                 hover_data=['total_restaurants', 'avg_rating'],
                 width=800, height=500,
                 labels={'avg_rating': 'Average Rating'},
                 title='Treemap of the 20 Most Popular European Cities Colored by Mean Rating',
                 color_continuous_scale='magma')

fig.update_layout(margin=dict(l=10, r=10, t=40, b=15))

fig.show()

"""Map Visualization"""

m = folium.Map(location=[51.216505, 9.879729], zoom_start=5, tiles="cartodbpositron")


for index, row in agg_top20_cities_df.iterrows():
    folium.CircleMarker(
        location=[row['latitude'], row['longitude']],
        radius=row['total_restaurants'] / 1000,
        color=px.colors.qualitative.Set1[0],
        fill=True,
        fill_color=px.colors.qualitative.Set1[0],
        fill_opacity=0.7,
        tooltip=f"{row['city']}<br>Average Rating: {row['avg_rating']}<br>Total Restaurants: {row['total_restaurants']}"
    ).add_to(m)

folium.LayerControl().add_to(m)

m

"""From this we can see that Paris has highest number of restaurants from all the cities

#Data Analysis

## Master Data Frame

Here we are creating a dataset which has information of top three famous cuisines from major countries and further we are analysing the data-set according to the following aspects:



*   Ratio of Local Restaurant
*   Average Price
*   Average Ratings
*   Average Reviews

As our analysis is based on localization of restaurants in Europe, we have eliminated some cuisines that represent regions or which are not related to Europe at all.
"""

lis=[' Vegetarian Friendly',' Gluten Free Options',' Mediterranean',' International',' Vegan Options',' Contemporary',' Asian',' Indonesian',' Japanese',' Seafood',' Fast Food',' American',' Bar',' Healthy',
' Indian',
' Tibetan',
' Nepali',
' Italian',
' Barbecue',
' Steakhouse',
' Latin',
' Argentinean',
' South American',
' Grill',
' Delicatessen',
' Thai',
' Soups',
' Street Food',
' Diner',
' Lebanese',
' Middle Eastern',
' Israeli',
' New Zealand',
' Chinese',
' Belgian',
' Sushi',
' Korean',
' Turkish',
' Vietnamese',
' Halal',
' Gastropub',
' Fusion',
' Arabic',
' Balti',
' Tunisian',
' Persian',
' Wine Bar',
' Mexican',
' Central American',
' Australian',
' Caribbean',
' African',
' Ethiopian',
' Brew Pub',
' Southwestern',
' Singaporean',
' Malaysian',
' Minority Chinese',
' Peruvian',
' Taiwanese',
' Hawaiian',
' Jamaican',
' Kosher',
' Brazilian',
' Pakistani',
' Afghani',
' Colombian',
' Ecuadorean',
' Cajun & Creole',
' Georgian',
' Egyptian',
' Central Asian',
' Cuban',
' Russian',
' Fujian',
' Czech',
' Armenian',
' Venezuelan',
' Bangladeshi',
' Azerbaijani',
' Polynesian',
' Albanian',
' Hungarian',
' Filipino',
' Yunnan',
' Cambodian',
' Chilean',
' Mongolian',
' Uzbek',
' Xinjiang',
' Ukrainian',
' Sri Lankan',
' Caucasian',
' Latvian',
' Salvadoran',
' Guatemalan',
' Native American',
' Canadian',
' Slovenian',
' Burmese',
' Puerto Rican']

"""Creating a list to keep only first cuisine from each country"""

for i in lis:
    df['cuisines'] = df['cuisines'].str.replace(fr"(^|,){i}(,|$)", "", regex=True)

df['cuisines'] = df['cuisines'].apply(lambda x: x.split(",")[0])

recommendations_df = df

# Here we created a dictionary to hold most common three cuisines for each country with their average ratings, average price range and average number of reviews.
# Also 4th index variable contains average values for all the values except for the most common cuisine to use it for further analysis.

countryDF = {}
for i in df.country.unique():
  countryDF[i] = []
for i in countryDF:
  cuis1 = df[df.country == i].groupby(['country', "cuisines"])["cuisines"].count().sort_values(ascending=False).reset_index(name="Count").iloc[0,1]
  cuis2 = df[df.country == i].groupby(['country', "cuisines"])["cuisines"].count().sort_values(ascending=False).reset_index(name="Count").iloc[1,1]
  cuis3 = df[df.country == i].groupby(['country', "cuisines"])["cuisines"].count().sort_values(ascending=False).reset_index(name="Count").iloc[2,1]
  rating1 = df[(df.country == i) & (df.cuisines == cuis1)].groupby("cuisines").mean().reset_index().iloc[0,5]
  rating2 = df[(df.country == i) & (df.cuisines == cuis2)].groupby("cuisines").mean().reset_index().iloc[0,5]
  rating3 = df[(df.country == i) & (df.cuisines == cuis3)].groupby("cuisines").mean().reset_index().iloc[0,5]
  rating4 = df[(df.country == i) & (df.cuisines != cuis1) & (df.cuisines !=cuis2) & (df.cuisines !=cuis3)].mean()[4]
  PriceRange1 = df[(df.country == i) & (df.cuisines == cuis1)].groupby("cuisines").mean().reset_index().iloc[0,7]
  PriceRange2 = df[(df.country == i) & (df.cuisines == cuis2)].groupby("cuisines").mean().reset_index().iloc[0,7]
  PriceRange3 = df[(df.country == i) & (df.cuisines == cuis3)].groupby("cuisines").mean().reset_index().iloc[0,7]
  PriceRange4 = df[(df.country == i) & (df.cuisines != cuis1) & (df.cuisines !=cuis2) & (df.cuisines !=cuis3)].mean()[6]
  numOfRevs1 = df[(df.country == i) & (df.cuisines == cuis1)].groupby("cuisines").mean().reset_index().iloc[0,6]
  numOfRevs2 = df[(df.country == i) & (df.cuisines == cuis2)].groupby("cuisines").mean().reset_index().iloc[0,6]
  numOfRevs3 = df[(df.country == i) & (df.cuisines == cuis3)].groupby("cuisines").mean().reset_index().iloc[0,6]
  numOfRevs4 = df[(df.country == i) & (df.cuisines != cuis1) & (df.cuisines !=cuis2) & (df.cuisines !=cuis3)].mean()[5]
  countryDF[i].append(cuis1)
  countryDF[i].append(cuis2)
  countryDF[i].append(cuis3)
  countryDF[i].append(rating1)
  countryDF[i].append(PriceRange1)
  countryDF[i].append(numOfRevs1)
  countryDF[i].append(rating2)
  countryDF[i].append(PriceRange2)
  countryDF[i].append(numOfRevs2)
  countryDF[i].append(rating3)
  countryDF[i].append(PriceRange3)
  countryDF[i].append(numOfRevs3)
  countryDF[i].append(rating4)
  countryDF[i].append(PriceRange4)
  countryDF[i].append(numOfRevs4)

#This dictionary is for holding the proportion of the most common three cuisiness

proportioncountry = {}
for i in df.country.unique():
  proportioncountry[i] = []
for i in proportioncountry:
  dfdummy = df[df.country == i].groupby(["country","cuisines"])["cuisines"].count().to_frame(name = "Count").sort_values(by = "Count", ascending = False).reset_index()
  prop1 = dfdummy.iloc[0,2] / dfdummy.Count.sum()
  prop2 = dfdummy.iloc[1,2] / dfdummy.Count.sum()
  prop3 = dfdummy.iloc[2,2] / dfdummy.Count.sum()
  proportioncountry[i].append(float(format(prop1*100,".0f")))
  proportioncountry[i].append(float(format(prop2*100,".0f")))
  proportioncountry[i].append(float(format(prop3*100,".0f")))

df_country = df.groupby(['country']).agg({"cuisines":pd.Series.mode, "avg_price":pd.Series.mean, "avg_rating":pd.Series.mean, "total_reviews_count": pd.Series.mean})

dfco = pd.DataFrame.from_dict(countryDF, orient = "index").reset_index()\
.rename(columns={"index": "country", 0 : "1stcuisines", 1 : "2ndcuisines", 2 : "3rdcuisines",
3 : "1stRating", 4 : "1stPrice", 5 : "1stReviews",
6 : "2ndRating", 7 : "2ndPrice", 8 : "2ndReviews",
9 : "3rdRating", 10 : "3rdPrice", 11 : "3rdReviews",
12 : "RestRating", 13 : "RestPrice", 14 : "RestReviews"})

dfpco = pd.DataFrame.from_dict(proportioncountry, orient = "index").reset_index()\
.rename(columns = {"index" : "country", 0 : "1st%", 1 : "2nd%", 2 : "3rd%"})

data2 = dfco.set_index('country').join(dfpco.set_index('country'))

#finally creating a master dataframe
countryLocalization = data2.join(df_country.reset_index().set_index("country"))\
.drop(columns = "cuisines").rename(columns = {"avg_price" : "AveragePrice", "Rating" : "AverageRating", "total_reviews_count" : "AverageReviews"})\
.reset_index()

df_country['total_reviews_count'] = df_country['total_reviews_count'].round(decimals=0)
df_country

dfco['1stReviews'] = dfco['1stReviews'].round(decimals=0)
dfco['2ndReviews'] = dfco['2ndReviews'].round(decimals=0)
dfco['3rdReviews'] = dfco['3rdReviews'].round(decimals=0)
dfco

"""This is what our master-dataframe looks like which holds the information of top 3 cuisines their average reviews, ratings, price

For each country, the proportions of the top three cuisines are stored in the proportioncountry dictionary under keys 1st%, 2nd%, and 3rd%.

These percentages represent the relative distribution of the top three cuisines which sum up to 100% for each country.
"""

countryLocalization

"""# Temporal and Spatial-map Visualization

##From the master data frame obtained we are visualizing it further according to the following aspects:






*   Ratio of Local Restaurant
*   Average Price
*   Average Ratings
*   Average Reviews
*   Relationships within Average Price and Average Rating
*   Relationships within Average Price and Average Reviews
"""

!pip install geopandas shapely

import geopandas as gpd
from shapely.geometry import Point
import geopandas as gpd
import pandas as pd
import folium
from folium import GeoJsonPopup

"""Converting .csv file to .geojson file to make the map visualization more interactive"""

csv_file_path = '/content/drive/MyDrive/countryLocalization.csv'
countryLocalization.to_csv(csv_file_path, index=False)

drive.mount('/content/drive')

world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))

csv_file_path = '/content/drive/MyDrive/countryLocalization.csv'
df = pd.read_csv(csv_file_path)

merged_df = world.merge(df, left_on='name', right_on='country')

geojson_file = 'countryLocalization.geojson'
merged_df.to_file(geojson_file, driver='GeoJSON')

print(f'Conversion completed. GeoJSON file saved at {geojson_file}')

gdf = gpd.read_file("countryLocalization.geojson")
gdf.head()

"""Using the inbuilt explore function of geo-pandas which builts a basic map of the data with all the information mentioned in the dataset"""

gdf.explore()

"""1. Local Restaurants Ratio"""

plt.figure(figsize = (15,6))
data = countryLocalization.loc[(countryLocalization.country != 'Finland')&(countryLocalization.country != 'Slovakia')&(countryLocalization.country != 'Luxembourg'), :].sort_values(by = "1st%", ascending = True)
bar = sns.barplot(data = data, x = "country", y = "1st%", palette = "PuOr")
plt.xticks(rotation = 90)
for p in bar.patches:
    bar.annotate(format(p.get_height(), '.1f'),
                   (p.get_x() + p.get_width() / 2., p.get_height()),
                   ha = 'center', va = 'center',
                   xytext = (0, 9),
                   textcoords = 'offset points')
plt.ylabel("Localization %")
plt.xlabel(None)
plt.show()

"""Creating new dataset with only country and local restaurents for map plotting"""

df = pd.DataFrame(gdf)
new_df = df[['country', '1st%']]

"""Using matplotlib to visualize the local restaurents"""

import geopandas as gpd
import matplotlib.pyplot as plt

fig, ax1 = plt.subplots(1, figsize=(10, 6))

gdf.plot(column='1st%', cmap='BuPu_r', linewidth=0.8, ax=ax1, edgecolor='0.8', legend=True)

ax1.axis('off')
ax1.set_title('Local Restaurants in each Country(%)', size=10)

sm = plt.cm.ScalarMappable(cmap='BuPu_r', norm=plt.Normalize(vmin=gdf['1st%'].min(), vmax=gdf['1st%'].max()))
sm._A = []
cbar = fig.colorbar(sm, ax=ax)

plt.show()

"""Using folium to make it more interactive"""

m = folium.Map(location=[51.216505, 9.879729],  tiles="OpenStreetMap", zoom_start=4)

cpleth = folium.Choropleth(gdf, data=gdf, key_on='feature.properties.country',
                          columns=['country', '1st%'],
                          fill_color="BuPu_r",
                          legend_name="Country, Local Restaurents",
                          name="Local Restaurents (%)")
tooltip = folium.features.GeoJsonTooltip(fields=['country', '1st%'],
                                        aliases=["Country", "Local Restaurents"],
                                        labels=True,
                                        stick=False)
cpleth.geojson.add_child(tooltip)
cpleth.add_to(m)
folium.LayerControl().add_to(m)
m

"""From this we can see that Italy has highest number of local restaurants i.e 59%

2. Prices
"""

plt.figure(figsize = (15,6))
data = countryLocalization.loc[(countryLocalization.country != 'Finland')&(countryLocalization.country != 'Slovakia')&(countryLocalization.country != 'Luxembourg'), :].sort_values(by = "AveragePrice", ascending = True)
bar = sns.barplot(data = data, x = "country", y = "AveragePrice", palette = "Blues")
plt.xticks(rotation = 35)
for p in bar.patches:
    bar.annotate(format(p.get_height(), '.1f'),
                   (p.get_x() + p.get_width() / 2., p.get_height()),
                   ha = 'center', va = 'center',
                   xytext = (0, 9),
                   textcoords = 'offset points')
plt.axhline(y=data.AveragePrice.mean(), color='Black', linestyle='--')
plt.ylim(1.4)
plt.ylabel("Local Cuisine Price")
plt.xlabel(None)
plt.show()

"""Create a new DataFrame with only 'country' and 'Average price' columns"""

df = pd.DataFrame(gdf)
new_df = df[['country', 'AveragePrice']]

"""Using matplotlib to visualize average price for each country"""

import geopandas as gpd
import matplotlib.pyplot as plt

fig, ax1 = plt.subplots(1, figsize=(10, 6))

gdf.plot(column='AveragePrice', cmap='Blues', linewidth=0.8, ax=ax1, edgecolor='0.8', legend=True)

ax1.axis('off')
ax1.set_title('Average price in Euroes for each Country', size=10)

sm = plt.cm.ScalarMappable(cmap='BuPu_r', norm=plt.Normalize(vmin=gdf['AveragePrice'].min(), vmax=gdf['AveragePrice'].max()))
sm._A = []
cbar = fig.colorbar(sm, ax=ax)

plt.show()

"""Using folium to make it more interactive"""

# Initialize a map centered on Europe
m = folium.Map(location=[51.216505, 9.879729],  tiles="OpenStreetMap", zoom_start=4)

cpleth = folium.Choropleth(gdf, data=gdf, key_on='feature.properties.country',
                          columns=['country', 'AveragePrice'],
                          fill_color="Blues",
                          legend_name="Country, Average Price",
                          name="Average Price ")
tooltip = folium.features.GeoJsonTooltip(fields=['country', 'AveragePrice'],
                                        aliases=["Country", "Average Price"],
                                        labels=True,
                                        stick=False)
cpleth.geojson.add_child(tooltip)
cpleth.add_to(m)
folium.LayerControl().add_to(m)
m

"""From this we can see that the most costly country is Denmark with average price of 40.5 euros and the most cheapest country is Poland with average price of 10.5 euros

3. Ratings
"""

plt.figure(figsize = (15,6))
data = countryLocalization.loc[(countryLocalization.country != 'Finland')&(countryLocalization.country != 'Slovakia')&(countryLocalization.country != 'Luxembourg'), :].sort_values(by = "avg_rating", ascending = True)
bar = sns.barplot(data = data, x = "country", y = "avg_rating", palette = "YlOrRd")
plt.xticks(rotation = 35)
for p in bar.patches:
    bar.annotate(format(p.get_height(), '.1f'),
                   (p.get_x() + p.get_width() / 2., p.get_height()),
                   ha = 'center', va = 'center',
                   xytext = (0, 9),
                   textcoords = 'offset points')
plt.axhline(y=data.avg_rating.mean(), color='Black', linestyle='--')
plt.ylim(3.5,5)
plt.ylabel("Local Rating")
plt.xlabel(None)
plt.show()

"""Create a new DataFrame with only 'country' and 'Average rating' columns"""

df = pd.DataFrame(gdf)
new_df = df[['country', 'avg_rating']]

"""Using matplotlib to visualize the rating of each country"""

import geopandas as gpd
import matplotlib.pyplot as plt



fig, ax1 = plt.subplots(1, figsize=(10, 6))


gdf.plot(column='avg_rating', cmap='YlOrRd', linewidth=0.8, ax=ax1, edgecolor='0.8', legend=True)

ax1.axis('off')
ax1.set_title('Average Rating out of 5 for each Country', size=10)

sm = plt.cm.ScalarMappable(cmap='YlOrRd', norm=plt.Normalize(vmin=gdf['avg_rating'].min(), vmax=gdf['avg_rating'].max()))
sm._A = []
cbar = fig.colorbar(sm, ax=ax)

plt.show()

"""Using folium to make it more interactive"""

# Initialize a map centered on Europe
m = folium.Map(location=[51.216505, 9.879729],  tiles="OpenStreetMap", zoom_start=4)

cpleth = folium.Choropleth(gdf, data=gdf, key_on='feature.properties.country',
                          columns=['country', 'avg_rating'],
                          fill_color="YlOrRd",
                          legend_name="Country, Local Ratings",
                          name="Local Ratings")
tooltip = folium.features.GeoJsonTooltip(fields=['country', 'avg_rating'],
                                        aliases=["Country", "Local Ratings"],
                                        labels=True,
                                        stick=False)
cpleth.geojson.add_child(tooltip)
cpleth.add_to(m)
folium.LayerControl().add_to(m)
m

"""From this we can see that Greece restaurants have highest ratings

4. Reviews
"""

plt.figure(figsize = (15,6))
data = countryLocalization.loc[(countryLocalization.country != 'Finland')&(countryLocalization.country != 'Slovakia')&(countryLocalization.country != 'Luxembourg'), :].sort_values(by = "AverageReviews", ascending = True)
bar = sns.barplot(data = data, x = "country", y = "AverageReviews", palette = "Purples")
plt.xticks(rotation = 35)
for p in bar.patches:
    bar.annotate(format(p.get_height(), '.1f'),
                   (p.get_x() + p.get_width() / 2., p.get_height()),
                   ha = 'center', va = 'center',
                   xytext = (0, 9),
                   textcoords = 'offset points')
plt.axhline(y=data.AverageReviews.mean(), color='Black', linestyle='--')
plt.ylabel("Local # of Reviews")
plt.xlabel(None)
plt.show()

"""
Create a new DataFrame with only 'country' and 'Average review' columns"""

df = pd.DataFrame(gdf)
new_df = df[['country', 'AverageReviews']]

"""Using matplotlib to visualize average reviews for each country"""

import geopandas as gpd
import matplotlib.pyplot as plt

fig, ax1 = plt.subplots(1, figsize=(10, 7))

gdf.plot(column='AverageReviews', cmap='Purples', linewidth=0.8, ax=ax1, edgecolor='0.8', legend=True)

ax1.axis('off')
ax1.set_title('Average Rating out of 5 for each Country', size=10)

sm = plt.cm.ScalarMappable(cmap='Purples', norm=plt.Normalize(vmin=gdf['AverageReviews'].min(), vmax=gdf['AverageReviews'].max()))
sm._A = []
cbar = fig.colorbar(sm, ax=ax)

plt.show()

"""Using folium to make it more interactive"""

# Initialize a map centered on Europe
m = folium.Map(location=[51.216505, 9.879729],  tiles="OpenStreetMap", zoom_start=4)

cpleth = folium.Choropleth(gdf, data=gdf, key_on='feature.properties.country',
                          columns=['country', 'AverageReviews'],
                          fill_color="Purples",
                          legend_name="Country, Local Reviews",
                          name="Local Reviews")
tooltip = folium.features.GeoJsonTooltip(fields=['country', 'AverageReviews'],
                                        aliases=["Country", "Local Reviews"],
                                        labels=True,
                                        stick=False)
cpleth.geojson.add_child(tooltip)
cpleth.add_to(m)
folium.LayerControl().add_to(m)
m

"""After looking up for average number of reviews done on local restaurants we can see that Italy is leading by far.

5. Relationships within Average Price and Average Rating
"""

sns.scatterplot(data=countryLocalization, x="AveragePrice", y="avg_rating", hue="country", palette="viridis", alpha=0.7)

sns.regplot(data=countryLocalization, x="AveragePrice", y="avg_rating", scatter=False, color='black')

plt.title('Relationships within Average Price and Average Rating')
plt.xlabel('Average Price')
plt.ylabel('Average Rating')

plt.show()

"""Here we got an intresting insight that as the average rating decreases the average price increases for a restaurant

6. Relationships within Average Price and Average Reviews
"""

sns.scatterplot(data=countryLocalization, x="AveragePrice", y="AverageReviews", hue="country", palette="viridis", alpha=0.7)

sns.regplot(data=countryLocalization, x="AveragePrice", y="AverageReviews", scatter=False, color='black')

plt.title('Relationships within Average Price and Average Reviews')
plt.xlabel('Average Price')
plt.ylabel('Average Reviews')

plt.show()

"""##Recommendation Syatem"""

import pandas as pd
from sklearn.neighbors import NearestNeighbors
from sklearn.preprocessing import LabelEncoder

restaurants_df = pd.DataFrame(recommendations_df)
restaurants_df.head()

restaurants_df1 = restaurants_df
restaurants_df2 = restaurants_df

restaurants_df1 = restaurants_df1.dropna(subset=['avg_rating', 'region'])

restaurants_df1.info()

#Label Encoding
label_encoder_country = LabelEncoder()
label_encoder_cuisine = LabelEncoder()
label_encoder_region = LabelEncoder()
label_encoder_price_level = LabelEncoder()
label_encoder_vegetarian_friendly = LabelEncoder()

#Features
restaurants_df1['country'] = label_encoder_country.fit_transform(restaurants_df1['country'])
restaurants_df1['cuisines'] = label_encoder_cuisine.fit_transform(restaurants_df1['cuisines'])
restaurants_df1['region'] = label_encoder_region.fit_transform(restaurants_df1['region'])
restaurants_df1['price_level'] = label_encoder_price_level.fit_transform(restaurants_df1['price_level'])
restaurants_df1['vegetarian_friendly'] = label_encoder_vegetarian_friendly.fit_transform(restaurants_df1['vegetarian_friendly'])


features = ['country', 'cuisines', 'region', 'price_level', 'vegetarian_friendly', 'open_days_per_week', 'avg_rating']

knn_model = NearestNeighbors(n_neighbors=3, metric='cosine')
knn_model.fit(restaurants_df1[features])

def get_recommendations(query_point, model, df, features, n_recommendations=3):
    distances, indices = model.kneighbors([query_point], n_neighbors=n_recommendations+2)
    recommendations = df.iloc[indices.flatten()][features]

   #inverse_transformed_cuisines = label_encoder.inverse_transform(recommendations['cuisines'])

    recommendations = df.iloc[indices.flatten()]

    recommendations['country'] = label_encoder_country.inverse_transform(recommendations['country'])
    recommendations['cuisines'] = label_encoder_cuisine.inverse_transform(recommendations['cuisines'])
    recommendations['region'] = label_encoder_region.inverse_transform(recommendations['region'])
    recommendations['price_level'] = label_encoder_price_level.inverse_transform(recommendations['price_level'])
    recommendations['vegetarian_friendly'] = label_encoder_vegetarian_friendly.inverse_transform(recommendations['vegetarian_friendly'])




    return recommendations

countries_options = restaurants_df2['country'].unique().tolist()

country_number = []
for i in range(1,len(countries_options)+1):
  country_number.append(i)

country_map = {}
country_map = dict(zip(country_number, countries_options))

country_map

"""### Entering User Inputs"""

## Write an input from the user for selected_country
selected_country = country_map[3]
print("Selected Country :", selected_country)

region_filter = restaurants_df2[restaurants_df2['country'] == selected_country]
unique_regions = region_filter['region'].unique().tolist()

region_number=[]
for i in range(1,len(unique_regions)+1):
  region_number.append(i)

region_map = {}
region_map = dict(zip(region_number, unique_regions))

region_map

selected_region = region_map[2]
print("Selected Region: ", selected_region)

## Write an input for selected region

cuisine_filter = restaurants_df2[restaurants_df2['region'] == selected_region]
unique_cuisines = cuisine_filter['cuisines'].unique().tolist()

cuisine_number = []
for i in range(1, len(unique_cuisines)+1):
  cuisine_number.append(i)

cuisine_map= {}
cuisine_map = dict(zip(cuisine_number, unique_cuisines))
print(cuisine_map)

selected_cuisine = cuisine_map[3]
print("Selected Cuisine: ", selected_cuisine)

price_level_unique = restaurants_df2['price_level'].unique().tolist()
price_level_numbers = [1,2,3]
price_level_map = dict(zip(price_level_numbers, price_level_unique))

price_level_map

print("Price Level:")
print(price_level_map)

selected_price_level = price_level_map[1]

print("Selected Price Level Option : ", selected_price_level)

vegetarian_friendly_unique = restaurants_df2['vegetarian_friendly'].unique().tolist()
vegetarian_friendly_numbers = [1,2]
vegetarian_friendly_map = dict(zip(vegetarian_friendly_numbers, vegetarian_friendly_unique))


print("Vegetarian Friendly (Yes/No)")
print(vegetarian_friendly_map)

selected_vegetarian_friendly = vegetarian_friendly_map[1]

print("Selected Vegetarian Friendly Option : ", selected_vegetarian_friendly)

user_query_country = label_encoder_country.transform([selected_country])[0]
user_query_cuisine = label_encoder_cuisine.transform([selected_cuisine])[0]
user_query_region = label_encoder_region.transform([selected_region])[0]
user_query_price_level = label_encoder_price_level.transform([selected_price_level])[0]
user_query_vegetarian_friendly = label_encoder_vegetarian_friendly.transform([selected_vegetarian_friendly])[0]
user_query_open_days_per_week = 7.0
user_query_avg_rating = 4.0

user_query = [user_query_country, user_query_cuisine, user_query_region, user_query_price_level, user_query_vegetarian_friendly, user_query_open_days_per_week, user_query_avg_rating]

print('The Requirements of the User: ')
print("Selected Country :", selected_country)
print("Selected Region: ", selected_region)
print("Selected Cuisine: ", selected_cuisine)
print("Selected Price Level Option : ", selected_price_level)
print("Vegetarian Friendly (Yes/No)")
print('Open Days per Week: ', user_query_open_days_per_week)
print('Average Rating :', user_query_avg_rating )

recommendations = get_recommendations(user_query, knn_model, restaurants_df1, features)
print("Top Restaurant Recommendations:")
#print(recommendations)
recommendations